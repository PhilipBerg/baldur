---
title: "Baldur Yeast Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Baldur Yeast Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# 1. Setup
This tutorial is quite fast and on a very simple data set (2 conditions only), for a more complicated tutorial for setup please see `vignette('baldur_ups_tutorial')`.
First we load `baldur` and setup the model dependent variables we need, then normalize the data and add the mean-variance trends.

```r
library(baldur)
# Setup design matrix
yeast_design <- model.matrix(~0+factor(rep(1:2, each = 3)))
colnames(yeast_design) <- paste0('ng', c(50, 100))
# Compare the first and second column of the design matrix
# with the following contrast matrix
yeast_contrast <- matrix(c(-1, 1), nrow = 2)

# Set id column
id_col <- colnames(yeast)[1] # "identifier"

# Since baldur itself does not deal with missing data we remove the
# rows that have missing data for the purpose of the tutorial.
# Else, one would replace the filtering step with imputation but that is outside
# the scope of baldur
yeast_norm <- yeast %>%
  # Remove missing data
  tidyr::drop_na() %>%
  # Normalize data (this might already have been done if imputation was performed)
  psrn(id_col) %>%
  # Add mean-variance trends
  calculate_mean_sd_trends(yeast_design)
```
Importantly, note that the column names of the design matrix are unique subsets of the names of the columns within the conditions:

```r
colnames(yeast)
#> [1] "identifier" "ng50_1"     "ng50_2"     "ng50_3"     "ng100_1"    "ng100_2"    "ng100_3"
colnames(yeast_design)
#> [1] "ng50"  "ng100"
```
This is essential for `baldur` to know which columns to use in calculations and to perform transformations.

# 2. Mean-Variance trends and Gamma Regression fitting
Next is to infer the mixture of the data and to estimate the parameters needed for `baldur`.
First we will setup the needed variables for using `baldur` without partitioning the data.
Then, partitioning and setting up `baldur` after trend-partitioning

```r
# Fit the gamma regression
gr_model <- fit_gamma_regression(yeast_norm, sd ~ mean)
# Estimate the uncertainty
unc_gr <- estimate_uncertainty(gr_model, yeast_norm, id_col, yeast_design)
```

# 3. Run the sampling procedure
Finally we sample the posterior of each row in the data.
First we sample assuming a single trend, then using the partitioning.

```r
# Single trend
gr_results <- gr_model %>%
  # Add hyper-priors for sigma
  estimate_gamma_hyperparameters(yeast_norm) %>%
  infer_data_and_decision_model(
    id_col,
    yeast_design,
    yeast_contrast,
    unc_gr,
    clusters = 10 # I highly recommend using parallel workers/clusters
  )               # this will greatly reduce the speed of running baldur
# The top hits then looks as follows:
gr_results %>%
  dplyr::arrange(err)
#> # A tibble: 1,802 × 22
#>    ident…¹ compa…²       err   lfc lfc_025 lfc_50 lfc_975 lfc_eff lfc_r…³  sigma sigma…⁴ sigma…⁵ sigma…⁶
#>    <chr>   <chr>       <dbl> <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>
#>  1 Cre09.… ng100 … 1.03e-194  6.19   5.77    6.19    6.60   1703.   1.00  0.155   0.0832  0.141   0.303 
#>  2 sp|P37… ng100 … 1.43e-187  1.51   1.41    1.51    1.62   3254.   1.00  0.0552  0.0298  0.0511  0.107 
#>  3 Cre12.… ng100 … 9.39e-176  1.61   1.49    1.61    1.72   2729.   0.999 0.0601  0.0325  0.0547  0.121 
#>  4 sp|P38… ng100 … 1.04e-144  1.07   0.992   1.07    1.16   3392.   1.00  0.0434  0.0232  0.0397  0.0845
#>  5 Cre14.… ng100 … 1.07e-132 -4.54  -4.90   -4.54   -4.16   3392.   1.00  0.176   0.0966  0.163   0.342 
#>  6 Cre10.… ng100 … 5.87e-123  4.16   3.80    4.16    4.51   3201.   1.00  0.182   0.101   0.168   0.345 
#>  7 Cre06.… ng100 … 4.41e- 97  4.20   3.80    4.20    4.61   2577.   1.00  0.180   0.0995  0.165   0.350 
#>  8 Cre12.… ng100 … 9.23e- 96  1.41   1.27    1.41    1.55   3188.   1.00  0.0712  0.0390  0.0652  0.137 
#>  9 sp|P09… ng100 … 2.27e- 92  1.47   1.32    1.47    1.62   2860.   1.00  0.0728  0.0391  0.0662  0.144 
#> 10 sp|P32… ng100 … 5.88e- 88  1.59   1.43    1.59    1.75   2937.   1.00  0.0839  0.0460  0.0766  0.167 
#> # … with 1,792 more rows, 9 more variables: sigma_eff <dbl>, sigma_rhat <dbl>, lp <dbl>, lp_025 <dbl>,
#> #   lp_50 <dbl>, lp_975 <dbl>, lp_eff <dbl>, lp_rhat <dbl>, warnings <list>, and abbreviated variable
#> #   names ¹​identifier, ²​comparison, ³​lfc_rhat, ⁴​sigma_025, ⁵​sigma_50, ⁶​sigma_975
```
Here `err` is the probability of error, i.e., the two tail-density supporting the null-hypothesis, `lfc` is the estimated log$_2$-fold change, `sigma` is the common variance, and `lp` is the log-posterior.
Columns without suffix shows the mean estimate from the posterior, while the suffixes `_025`, `_50`, and `_975`, are the 2.5, 50.0, and 97.5, percentiles, respectively.
The suffixes `_eff` and `_rhat` are the diagnostic variables returned by `rstan` (please see the Stan manual for details).
In general, a larger `_eff` indicates a better sampling efficiency, and `_rhat` compares the mixing within chains against between the chains and should be smaller than 1.05.

# 4. Running Baldur with Latent Gamma Mixture Regression estimation
First we fit the LGMR model:

```r
yeast_lgmr <- fit_lgmr(yeast_norm, id_col, lgmr_model, cores = 5)
```

We can print the model with `print` and extract parameters of interest with `coef`:

```r
print(yeast_lgmr, pars = c("coef", "aux"))
#> 
#> LGMR Model
#> 	mu=exp(-1.798615 - 0.3221564 f(bar_y)) + kappa exp(7.06277 - 0.3838847 f(bar_y))
#> 
#>  Auxiliary:
#>         mean   se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff  Rhat
#> alpha  4.095  0.004255  0.234  3.656  3.933  4.090  4.250  4.565   3031     1
#> nrmse  0.562  0.000234  0.014  0.534  0.552  0.562  0.571  0.589   3572     1
#> 
#> 
#>  Coefficients:
#>        mean   se_mean      sd    2.5%     25%     50%     75%   97.5%  n_eff  Rhat
#> I    -1.799  0.000420  0.0258  -1.848  -1.816  -1.799  -1.782  -1.747   3779     1
#> I_L   7.063  0.000485  0.0417   6.981   7.035   7.062   7.091   7.145   7384     1
#> S     0.322  0.000258  0.0231   0.277   0.307   0.322   0.338   0.369   8079     1
#> S_L   0.384  0.000281  0.0347   0.316   0.360   0.384   0.408   0.452  15248     1
# Extract the regression, alpha, and theta parameters and the NRMSE.
yeast_lgmr_coef <- coef(yeast_lgmr, pars = "all")
```

We can then estimate the uncertainty similar to the GR case:

```r
unc_lgmr <- estimate_uncertainty(yeast_lgmr, yeast_norm, id_col, yeast_design)
```

Then running the data and decision model:

```r
# Single trend
lgmr_results <- yeast_lgmr %>%
  # Add hyper-priors for sigma
  estimate_gamma_hyperparameters(yeast_norm, id_col) %>%
  infer_data_and_decision_model(
    id_col,
    yeast_design,
    yeast_contrast,
    unc_lgmr,
    clusters = 10
  )
```

# 5. Visualization of the results
`baldur` have two ways of visualizing the results 1) plotting sigma vs LFC and 2) Volcano plots.
To plot sigma against LFC we use `plot_sa`:

```r
gr_results %>%
  plot_sa(
    alpha = .05, # Level of significance
    lfc = 1      # Add LFC lines
  )

lgmr_results %>%
  plot_sa(
    alpha = .05, # Level of significance
    lfc = 1      # Add LFC lines
  )
```

![plot of chunk plotting_sa](plotting_sa-1yeast.png)![plot of chunk plotting_sa](plotting_sa-2yeast.png)

While it is hard to see with this few examples, in general a good decision is indicated by a lack of a trend between $\sigma$ and LFC.
To make a volcano plot one uses `plot_volcano` in a similar fashion to `plot_sa`:

```r
gr_results %>%
  plot_volcano(
    alpha = .05, # Level of significance
    lfc = 1      # Add LFC lines
  )

lgmr_results %>%
  plot_volcano(
    alpha = .05, # Level of significance
    lfc = 1      # Add LFC lines
  )
```

![plot of chunk plotting_volc](plotting_volc-1yeast.png)![plot of chunk plotting_volc](plotting_volc-2yeast.png)
